{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.use_inf_as_null = True\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descriptor_target_split(file):\n",
    "    \"\"\"\n",
    "    Split the input data into descriptors and target DataFrames\n",
    "        Parameters:\n",
    "            file: pandas.DataFrame\n",
    "                Input DataFrame containing descriptors and target data.\n",
    "        Returns:\n",
    "            descriptors: pandas.DataFrame\n",
    "                Descriptors DataFrame.\n",
    "            target: pandas.DataFrame\n",
    "                Target DataFrame.\n",
    "    \"\"\"\n",
    "    target = file.loc[:, file.columns == 'Target']\n",
    "    descriptors = file.loc[:, file.columns != 'Target']\n",
    "    return descriptors, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descriptor_target_join(descriptors,target):\n",
    "    \"\"\"\n",
    "    Merge the Descriptors and Target DataFrames\n",
    "        Parameters:\n",
    "            descriptors: pandas.DataFrame\n",
    "                Descriptors DataFrame.\n",
    "            target: pandas.DataFrame\n",
    "                Target DataFrame.\n",
    "        Returns:\n",
    "            file: pandas.DataFrame\n",
    "                Input DataFrame containing descriptors and target data.\n",
    "    \"\"\"\n",
    "\n",
    "    descriptors['Target'] = target['Target']\n",
    "    file = descriptors\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_imputation(file,missing_value_type = \"NaN\",strategy = \"mean\", axis = 0):\n",
    "    \"\"\"\n",
    "    Imputes placeholder missing values in data\n",
    "        Parameters:\n",
    "            file: {array-like, sparse matrix}\n",
    "                Sample vectors which may have have missing values\n",
    "            missing_value_type:  string, optional (default=\"NaN\")\n",
    "                Placeholder for missing value. If none is given, \"NaN\" will be used.\n",
    "            strategy = string, optional (default=\"mean\")\n",
    "                Strategy for replacing missing values. It must be one of \"mean\", \"median\", or \"mode\". If none is given,\n",
    "                \"mean\" is used\n",
    "            axis = int, optional (default=0)\n",
    "                Imputations along rows or columns. It must be one of 0 (for columns) or 1 (for rows)\n",
    "        Returns:\n",
    "            file: {array-like, sparse matrix}\n",
    "    \"\"\"\n",
    "    file.replace(missing_value_type,np.nan,inplace = True)\n",
    "    #Replacing None and np.nan with the given strategy\n",
    "    if axis == 0:\n",
    "        if strategy == \"mean\":\n",
    "            for i in list(file.columns):\n",
    "                file[i].fillna(file[i].mean(), inplace=True)\n",
    "        if strategy == \"median\":\n",
    "            for i in list(file.columns):\n",
    "                file[i].fillna(file[i].median(), inplace=True)\n",
    "        if strategy == \"mode\":\n",
    "            for i in list(file.columns):\n",
    "                file[i].fillna(file[i].mode(), inplace=True)\n",
    "    \n",
    "    elif axis == 1:\n",
    "        if strategy == \"mean\":\n",
    "                file = file.T.fillna(file.mean(axis=1)).T\n",
    "        if strategy == \"median\":\n",
    "                file = file.T.fillna(file.median(axis=1)).T\n",
    "        if strategy == \"mode\":\n",
    "                file = file.T.fillna(file.mode(axis=1)).T\n",
    "        \n",
    "    else:\n",
    "        print('Axis value incorrect')\n",
    "        return\n",
    "    \n",
    "    if file.isnull().sum().sum() == 0:\n",
    "        return file\n",
    "    else:\n",
    "        print('Missing values present')\n",
    "        return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_variance_features(file,threshold_value = 0.01):\n",
    "    \"\"\"\n",
    "    Feature selector that removes all low-variance features.\n",
    "        Parameters:\n",
    "            file: pandas.DataFrame\n",
    "                Input Data from which to compute variances.\n",
    "            threshold : float, optional\n",
    "                Features with a training-set variance lower than this threshold will be removed. \n",
    "        Returns:\n",
    "            file: {array-like, sparse matrix}\n",
    "                Transformed array.\n",
    "    \"\"\"\n",
    "    column_list = list(file.columns)\n",
    "    selector = VarianceThreshold(threshold_value)\n",
    "    transformed_arrays = selector.fit_transform(file)\n",
    "    transformed_columns_list = [column_list[i] for i in selector.get_support(indices = True)]\n",
    "    file = pd.DataFrame(transformed_arrays,columns = transformed_columns_list)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_high_correlated_features(file,threshold_value):\n",
    "    \"\"\"\n",
    "    Feature selector that removes all highly-correlated features.\n",
    "        Parameters:\n",
    "            file: pandas.DataFrame\n",
    "                Input DataFrame to remove highly correlated features.\n",
    "            threshold : float, optional\n",
    "                Features with a correlation higher than this threshold will be removed. \n",
    "        Returns:\n",
    "            file: {array-like, sparse matrix}\n",
    "                Transformed array.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def univariate_feature_selection(file,k_value = 10,score_function = \"f_regression\"):\n",
    "    \"\"\"\n",
    "    Univariate feature selection works by selecting the best features based on univariate statistical tests. \n",
    "    Selects features according to the k highest scores.\n",
    "        Parameters:\n",
    "            file: pandas.DataFrame\n",
    "                Input DataFrame to perform univariate feature selection.\n",
    "            k_value: int, optional, default=10\n",
    "                Number of top features to select.\n",
    "            score_function: string, optional, default=\"f_regression\"\n",
    "                Scoring function that return scores and pvalues. It must be one of \"f_regression\" or \"mutual_info_regression\". \n",
    "                If none is given, \"f_regression\" is used\n",
    "        Returns:\n",
    "            file: {array-like, sparse matrix}\n",
    "                Transformed array.\n",
    "    \"\"\"\n",
    "    if score_function == \"f_regression\":\n",
    "        from sklearn.feature_selection import f_regression\n",
    "        selector = SelectKBest(f_regression,k_value)\n",
    "    elif score_function == \"mutual_info_regression\":\n",
    "        from sklearn.feature_selection import mutual_info_regression\n",
    "        selector = SelectKBest(mutual_info_regression,k_value)\n",
    "    descriptors, target = descriptor_target_split(file)\n",
    "    column_list = list(descriptors.columns)\n",
    "    transformed_arrays = selector.fit_transform(descriptors,target)\n",
    "    transformed_columns_list = [column_list[i] for i in selector.get_support(indices = True)]\n",
    "    file = pd.DataFrame(transformed_arrays,columns = transformed_columns_list)\n",
    "    file = descriptor_target_join(file,target)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tree_based_feature_selection(file,n_estimators_value = 10,max_features_value = None,threshold_value = \"mean\"):\n",
    "    \"\"\"\n",
    "    Feature selection using a tree-based estimator to compute feature importances, which in turn can be used \n",
    "    to discard irrelevant features\n",
    "        Parameters:\n",
    "            file: pandas.DataFrame\n",
    "                Input DataFrame to perform tree based feature selection.\n",
    "            n_estimators: int, optional, default=10\n",
    "                Number of trees in the forest.\n",
    "            max_features_value: {int, float, string}, optional, default=None\n",
    "                The number of features to consider when looking for the best split.\n",
    "                If int, then consider max_features_value features at each split.\n",
    "                If float, then max_features_value is a percentage and int(max_features_value*n_features) features are \n",
    "                considered at each split.\n",
    "                If \"auto\", then max_features_value=sqrt(n_features)\n",
    "                If \"sqrt\", then max_features_value=sqrt(n_features)\n",
    "                If \"log2\", then max_features_value=log2(n_features)\n",
    "                If None, then max_features_value=n_features\n",
    "            threshold_value: {int, string}, optional, default=\"mean\"\n",
    "                The threshold value to use for feature selection. Features whose importance is greater or equal are kept while \n",
    "                the others are discarded. It must be one of \"1.25*mean\", \"median\", \"1e-5\" or \"0.001\". If none is given, \n",
    "                \"mean\" is used\n",
    "        Returns:\n",
    "            file: {array-like, sparse matrix}\n",
    "                Transformed array.    \n",
    "    \"\"\"\n",
    "    descriptors, target = descriptor_target_split(file)\n",
    "    column_list = list(descriptors.columns)\n",
    "    clf = ExtraTreesClassifier(n_estimators = n_estimators_value, max_features = max_features_value)\n",
    "    clf = clf.fit(descriptors, target)\n",
    "    model = SelectFromModel(clf, prefit=True,threshold=threshold_value)\n",
    "    transformed_arrays = model.transform(descriptors)\n",
    "    transformed_columns_list = [column_list[i] for i in model.get_support(indices = True)]\n",
    "    file = pd.DataFrame(transformed_arrays,columns = transformed_columns_list)\n",
    "    file = descriptor_target_join(file,target)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_feature_selection(file, step_value = 1, max_features_value = 3):\n",
    "    \"\"\"\n",
    "    Select features by recursively considering smaller and smaller sets of features. \n",
    "        Parameters:\n",
    "            file: pandas.DataFrame\n",
    "                Input DataFrame to perform RFE based feature selection.\n",
    "            step_value: int, optional, default=1\n",
    "                If greater than or equal to 1, then step corresponds to the (integer) number of features to remove at each \n",
    "                iteration. If within (0.0, 1.0), then step corresponds to the percentage (rounded down) of features to \n",
    "                remove at each iteration.\n",
    "            max_features_value: int, optional, default=3\n",
    "                Number of trees in the forest.\n",
    "        Returns:\n",
    "            file: {array-like, sparse matrix}\n",
    "                Transformed array.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_train_split(file, test_size_value = 0.25, train_size_value = None):\n",
    "    \"\"\"\n",
    "    Split Input DataFrame into Training and Testing Data\n",
    "        Parameters:\n",
    "            file: pandas.DataFrame\n",
    "                Input DataFrame containing descriptors and target data.\n",
    "            test_size : float, int, default=0.25\n",
    "                If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the \n",
    "                test split. If int, represents the absolute number of test samples. If None, the value is set to the \n",
    "                complement of the train size.\n",
    "            train_size : float, int, or None, default None\n",
    "                If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the \n",
    "                train split. If int, represents the absolute number of train samples. If None, the value is automatically \n",
    "                set to the complement of the test size.\n",
    "        Returns:\n",
    "            train: pandas.DataFrame\n",
    "                DataFrame containing training data.\n",
    "            test: pandas.DataFrame\n",
    "                DataFrame containing testing data.\n",
    "    \"\"\"\n",
    "    descriptors, target = descriptor_target_split(file)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(descriptors, target, test_size=test_size_value, train_size=train_size_value)\n",
    "    train = descriptor_target_join(X_train,y_train)\n",
    "    train.reset_index(inplace = True)\n",
    "    train.drop('index',axis = 1,inplace = True)\n",
    "    test = descriptor_target_join(X_test,y_test)\n",
    "    test.reset_index(inplace = True)\n",
    "    test.drop('index',axis = 1,inplace = True)\n",
    "    return train,test"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
